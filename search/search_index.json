{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"video_features allows you to extract features from raw videos in parallel with multiple GPUs. It supports several extractors that capture visual appearance, optical flow, and audio features. Quick Start # clone the repo and change the working directory git clone https://github.com/v-iashin/video_features.git cd video_features # install environment conda env create -f conda_env_torch_zoo.yml # load the environment conda activate torch_zoo # extract r(2+1)d features for the sample videos python main.py \\ feature_type=r21d \\ device_ids=\"[0]\" \\ video_paths=\"[./sample/v_ZNVhz7ctTq0.mp4, ./sample/v_GGSY1Qvo990.mp4]\" # use `device_ids=\"[0, 2]\"` to run on 0th and 2nd devices in parallel Supported Models Action Recognition I3D-Net RGB + Flow (Kinetics 400) R(2+1)d RGB (IG-65M, Kinetics 400) Sound Recognition VGGish (AudioSet) Optical Flow RAFT (FlyingChairs, FlyingThings3D, Sintel, KITTI) PWC-Net (Sintel) Image Recognition ResNet-18,34,50,101,152 (ImageNet)","title":"Home"},{"location":"#quick-start","text":"# clone the repo and change the working directory git clone https://github.com/v-iashin/video_features.git cd video_features # install environment conda env create -f conda_env_torch_zoo.yml # load the environment conda activate torch_zoo # extract r(2+1)d features for the sample videos python main.py \\ feature_type=r21d \\ device_ids=\"[0]\" \\ video_paths=\"[./sample/v_ZNVhz7ctTq0.mp4, ./sample/v_GGSY1Qvo990.mp4]\" # use `device_ids=\"[0, 2]\"` to run on 0th and 2nd devices in parallel","title":"Quick Start"},{"location":"#supported-models","text":"Action Recognition I3D-Net RGB + Flow (Kinetics 400) R(2+1)d RGB (IG-65M, Kinetics 400) Sound Recognition VGGish (AudioSet) Optical Flow RAFT (FlyingChairs, FlyingThings3D, Sintel, KITTI) PWC-Net (Sintel) Image Recognition ResNet-18,34,50,101,152 (ImageNet)","title":"Supported Models"},{"location":"models/i3d/","text":"I3D (RGB + Flow) The Inflated 3D ( I3D ) features are extracted using a pre-trained model on Kinetics 400 . Here, the features are extracted from the second-to-the-last layer of I3D, before summing them up. Therefore, it outputs two tensors with 1024-d features: for RGB and flow streams. By default, it expects to input 64 RGB and flow frames ( 224x224 ) which spans 2.56 seconds of the video recorded at 25 fps. In the default case, the features will be of size Tv x 1024 where Tv = duration / 2.56 . Please note, this implementation uses either PWC-Net (the default) and RAFT optical flow extraction instead of the TV-L1 algorithm, which was used in the original I3D paper as it hampers speed. Yet, it might possibly lead to worse peformance. Our tests show that the performance is reasonable. You may test it yourself by providing --show_pred flag. CUDA 11 and GPUs like RTX 3090 and newer PWC optical flow back-end is not supported on CUDA 11 and, therefore, GPUs like RTX 3090 and newer. RGB-only model should still work. For details please check this issue #13 If you were able to fix it, please share your workaround. Feel free to use flow_type=raft RAFT during extraction. Set up the Environment for I3D Depending on whether you would like to use PWC-Net or RAFT for optical flow extraction, you will need to install separate conda environments \u2013 conda_env_pwc.yml and conda_env_torch_zoo , respectively # it will create a new conda environment called 'pwc' on your machine conda env create -f conda_env_pwc.yml # or/and if you would like to extract optical flow with RAFT conda env create -f conda_env_torch_zoo.yml Minimal Working Example Activate the environment conda activate pwc if you would like to use RAFT as optical flow extractor use torch_zoo instead of pwc : and extract features from ./sample/v_ZNVhz7ctTq0.mp4 video and show the predicted classes python main.py \\ feature_type=i3d \\ video_paths=\"[./sample/v_ZNVhz7ctTq0.mp4]\" \\ show_pred=true Examples Activate the environment conda activate pwc The following will extract I3D features for sample videos using 0th and 2nd devices in parallel. The features are going to be extracted with the default parameters. python main.py \\ feature_type=i3d \\ device_ids=\"[0, 2]\" \\ video_paths=\"[./sample/v_ZNVhz7ctTq0.mp4, ./sample/v_GGSY1Qvo990.mp4]\" The video paths can be specified as a .txt file with paths python main.py \\ feature_type=i3d \\ device_ids=\"[0, 2]\" \\ file_with_video_paths=./sample/sample_video_paths.txt It is also possible to extract features from either rgb or flow modalities individually ( --streams ) and, therefore, increasing the speed python main.py \\ feature_type=i3d \\ streams=flow \\ device_ids=\"[0, 2]\" \\ file_with_video_paths=./sample/sample_video_paths.txt To extract optical flow frames using RAFT approach, specify --flow_type raft . Note that using RAFT will make the extraction slower than with PWC-Net yet visual inspection of extracted flow frames suggests that RAFT has a better quality of the estimated flow # make sure to activate the correct environment (`torch_zoo`) # conda activate torch_zoo python main.py \\ feature_type=i3d \\ flow_type=raft \\ device_ids=\"[0, 2]\" \\ file_with_video_paths=./sample/sample_video_paths.txt The features can be saved as numpy arrays by specifying --on_extraction save_numpy or save_pickle . By default, it will create a folder ./output and will store features there python main.py \\ feature_type=i3d \\ device_ids=\"[0, 2]\" \\ on_extraction=save_numpy \\ file_with_video_paths=./sample/sample_video_paths.txt You can change the output folder using --output_path argument. Also, you may want to try to change I3D window and step sizes python main.py \\ feature_type=i3d \\ device_ids=\"[0, 2]\" \\ stack_size=24 \\ step_size=24 \\ file_with_video_paths=./sample/sample_video_paths.txt By default, the frames are extracted according to the original fps of a video. If you would like to extract frames at a certain fps, specify --extraction_fps argument. python main.py \\ feature_type=i3d \\ device_ids=\"[0, 2]\" \\ extraction_fps=25 \\ stack_size=24 \\ step_size=24 \\ file_with_video_paths=./sample/sample_video_paths.txt A fun note, the time span of the I3D features in the last example will match the time span of VGGish features with default parameters (24/25 = 0.96). If --keep_tmp_files is specified, it keeps them in --tmp_path which is ./tmp by default. Be careful with the --keep_tmp_files argument when playing with --extraction_fps as it may mess up the frames you extracted before in the same folder. Credits An implementation of PWC-Net in PyTorch The Official RAFT implementation (esp. ./demo.py ) . A port of I3D weights from TensorFlow to PyTorch The I3D paper: Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset . License The wrapping code is MIT and the port of I3D weights from TensorFlow to PyTorch. However, PWC Net (default flow extractor) has GPL-3.0 and RAFT BSD 3-Clause .","title":"I3D (RGB + Flow)"},{"location":"models/i3d/#i3d-rgb-flow","text":"The Inflated 3D ( I3D ) features are extracted using a pre-trained model on Kinetics 400 . Here, the features are extracted from the second-to-the-last layer of I3D, before summing them up. Therefore, it outputs two tensors with 1024-d features: for RGB and flow streams. By default, it expects to input 64 RGB and flow frames ( 224x224 ) which spans 2.56 seconds of the video recorded at 25 fps. In the default case, the features will be of size Tv x 1024 where Tv = duration / 2.56 . Please note, this implementation uses either PWC-Net (the default) and RAFT optical flow extraction instead of the TV-L1 algorithm, which was used in the original I3D paper as it hampers speed. Yet, it might possibly lead to worse peformance. Our tests show that the performance is reasonable. You may test it yourself by providing --show_pred flag. CUDA 11 and GPUs like RTX 3090 and newer PWC optical flow back-end is not supported on CUDA 11 and, therefore, GPUs like RTX 3090 and newer. RGB-only model should still work. For details please check this issue #13 If you were able to fix it, please share your workaround. Feel free to use flow_type=raft RAFT during extraction.","title":"I3D (RGB + Flow)"},{"location":"models/i3d/#set-up-the-environment-for-i3d","text":"Depending on whether you would like to use PWC-Net or RAFT for optical flow extraction, you will need to install separate conda environments \u2013 conda_env_pwc.yml and conda_env_torch_zoo , respectively # it will create a new conda environment called 'pwc' on your machine conda env create -f conda_env_pwc.yml # or/and if you would like to extract optical flow with RAFT conda env create -f conda_env_torch_zoo.yml","title":"Set up the Environment for I3D"},{"location":"models/i3d/#minimal-working-example","text":"Activate the environment conda activate pwc if you would like to use RAFT as optical flow extractor use torch_zoo instead of pwc : and extract features from ./sample/v_ZNVhz7ctTq0.mp4 video and show the predicted classes python main.py \\ feature_type=i3d \\ video_paths=\"[./sample/v_ZNVhz7ctTq0.mp4]\" \\ show_pred=true","title":"Minimal Working Example"},{"location":"models/i3d/#examples","text":"Activate the environment conda activate pwc The following will extract I3D features for sample videos using 0th and 2nd devices in parallel. The features are going to be extracted with the default parameters. python main.py \\ feature_type=i3d \\ device_ids=\"[0, 2]\" \\ video_paths=\"[./sample/v_ZNVhz7ctTq0.mp4, ./sample/v_GGSY1Qvo990.mp4]\" The video paths can be specified as a .txt file with paths python main.py \\ feature_type=i3d \\ device_ids=\"[0, 2]\" \\ file_with_video_paths=./sample/sample_video_paths.txt It is also possible to extract features from either rgb or flow modalities individually ( --streams ) and, therefore, increasing the speed python main.py \\ feature_type=i3d \\ streams=flow \\ device_ids=\"[0, 2]\" \\ file_with_video_paths=./sample/sample_video_paths.txt To extract optical flow frames using RAFT approach, specify --flow_type raft . Note that using RAFT will make the extraction slower than with PWC-Net yet visual inspection of extracted flow frames suggests that RAFT has a better quality of the estimated flow # make sure to activate the correct environment (`torch_zoo`) # conda activate torch_zoo python main.py \\ feature_type=i3d \\ flow_type=raft \\ device_ids=\"[0, 2]\" \\ file_with_video_paths=./sample/sample_video_paths.txt The features can be saved as numpy arrays by specifying --on_extraction save_numpy or save_pickle . By default, it will create a folder ./output and will store features there python main.py \\ feature_type=i3d \\ device_ids=\"[0, 2]\" \\ on_extraction=save_numpy \\ file_with_video_paths=./sample/sample_video_paths.txt You can change the output folder using --output_path argument. Also, you may want to try to change I3D window and step sizes python main.py \\ feature_type=i3d \\ device_ids=\"[0, 2]\" \\ stack_size=24 \\ step_size=24 \\ file_with_video_paths=./sample/sample_video_paths.txt By default, the frames are extracted according to the original fps of a video. If you would like to extract frames at a certain fps, specify --extraction_fps argument. python main.py \\ feature_type=i3d \\ device_ids=\"[0, 2]\" \\ extraction_fps=25 \\ stack_size=24 \\ step_size=24 \\ file_with_video_paths=./sample/sample_video_paths.txt A fun note, the time span of the I3D features in the last example will match the time span of VGGish features with default parameters (24/25 = 0.96). If --keep_tmp_files is specified, it keeps them in --tmp_path which is ./tmp by default. Be careful with the --keep_tmp_files argument when playing with --extraction_fps as it may mess up the frames you extracted before in the same folder.","title":"Examples"},{"location":"models/i3d/#credits","text":"An implementation of PWC-Net in PyTorch The Official RAFT implementation (esp. ./demo.py ) . A port of I3D weights from TensorFlow to PyTorch The I3D paper: Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset .","title":"Credits"},{"location":"models/i3d/#license","text":"The wrapping code is MIT and the port of I3D weights from TensorFlow to PyTorch. However, PWC Net (default flow extractor) has GPL-3.0 and RAFT BSD 3-Clause .","title":"License"},{"location":"models/pwc/","text":"PWC-Net PWC-Net: CNNs for Optical Flow Using Pyramid, Warping, and Cost Volume frames are extracted for every consecutive pair of frames in a video. PWC-Net is pre-trained on Sintel Flow dataset . The implementation follows sniklaus/pytorch-pwc@f61389005 . CUDA 11 and GPUs like RTX 3090 and newer The current environment does not support CUDA 11 and, therefore, GPUs like RTX 3090 and newer. For details please check this issue #13 If you were able to fix it, please share your workaround. If you need an optical flow extractor, you are recommended to use RAFT . Set up the Environment for PWC Setup conda environment. # it will create a new conda environment called 'pwc' on your machine conda env create -f conda_env_pwc.yml Minimal Working Example Activate the environment conda activate pwc and extract optical flow from ./sample/v_GGSY1Qvo990.mp4 using one GPU and show the flow for each frame python main.py \\ feature_type=pwc \\ show_pred=true \\ video_paths=\"[./sample/v_GGSY1Qvo990.mp4]\" Note , if show_pred=true , the window with predictions will appear, use any key to show the next frame. To use show_pred=true , a screen must be attached to the machine or X11 forwarding is enabled. Examples Please see the examples for RAFT optical flow frame extraction. Make sure to replace --feature_type argument to pwc . Credits The PWC-Net paper and official implementation . The PyTorch implementation used in this repo . License The wrapping code is under MIT, but PWC Net has GPL-3.0","title":"PWC-Net"},{"location":"models/pwc/#pwc-net","text":"PWC-Net: CNNs for Optical Flow Using Pyramid, Warping, and Cost Volume frames are extracted for every consecutive pair of frames in a video. PWC-Net is pre-trained on Sintel Flow dataset . The implementation follows sniklaus/pytorch-pwc@f61389005 . CUDA 11 and GPUs like RTX 3090 and newer The current environment does not support CUDA 11 and, therefore, GPUs like RTX 3090 and newer. For details please check this issue #13 If you were able to fix it, please share your workaround. If you need an optical flow extractor, you are recommended to use RAFT .","title":"PWC-Net"},{"location":"models/pwc/#set-up-the-environment-for-pwc","text":"Setup conda environment. # it will create a new conda environment called 'pwc' on your machine conda env create -f conda_env_pwc.yml","title":"Set up the Environment for PWC"},{"location":"models/pwc/#minimal-working-example","text":"Activate the environment conda activate pwc and extract optical flow from ./sample/v_GGSY1Qvo990.mp4 using one GPU and show the flow for each frame python main.py \\ feature_type=pwc \\ show_pred=true \\ video_paths=\"[./sample/v_GGSY1Qvo990.mp4]\" Note , if show_pred=true , the window with predictions will appear, use any key to show the next frame. To use show_pred=true , a screen must be attached to the machine or X11 forwarding is enabled.","title":"Minimal Working Example"},{"location":"models/pwc/#examples","text":"Please see the examples for RAFT optical flow frame extraction. Make sure to replace --feature_type argument to pwc .","title":"Examples"},{"location":"models/pwc/#credits","text":"The PWC-Net paper and official implementation . The PyTorch implementation used in this repo .","title":"Credits"},{"location":"models/pwc/#license","text":"The wrapping code is under MIT, but PWC Net has GPL-3.0","title":"License"},{"location":"models/r21d/","text":"R(2+1)D (RGB-only) We support 3 flavors of R(2+1)D: r2plus1d_18_16_kinetics 18-layer R(2+1)D pre-trained on Kinetics 400 (used by default) \u2013 it is identical to the torchvision implementation r2plus1d_34_32_ig65m_ft_kinetics 34-layer R(2+1)D pre-trained on IG-65M and fine-tuned on Kinetics 400 \u2013 the weights are provided by moabitcoin/ig65m-pytorch repo for stack/step size 32 . r2plus1d_34_8_ig65m_ft_kinetics the same as the one above but this one was pre-trained with stack/step size 8 models are pre-trained on RGB frames and follow the plain torchvision augmentation sequence . Info The flavors that were pre-trained on IG-65M and fine-tuned on Kinetics 400 yield significantly better performance than the default model (e.g. the 32 frame model reaches an accuracy of 79.10 vs 57.50 by default). By default ( --model_name=r2plus1d_18_16_kinetics ), the model expects to input a stack of 16 RGB frames ( 112x112 ), which spans 0.64 seconds of the video recorded at 25 fps. In the default case, the features will be of size Tv x 512 where Tv = duration / 0.64 . Specify, model_name , --step_size and --stack_size to change the default behavior. Set up the Environment for R(2+1)D Setup conda environment. Requirements are in file conda_env_torch_zoo.yml # it will create a new conda environment called 'torch_zoo' on your machine conda env create -f conda_env_torch_zoo.yml Minimal Working Example Activate the environment conda activate torch_zoo and extract features from the ./sample/v_GGSY1Qvo990.mp4 video and show the predicted classes python main.py \\ feature_type=r21d \\ video_paths=\"[./sample/v_GGSY1Qvo990.mp4]\" \\ show_pred=true Example Start by activating the environment conda activate torch_zoo It will extract R(2+1)d features for sample videos using 0th and 2nd devices in parallel. The features are going to be extracted with the default parameters. python main.py \\ feature_type=r21d \\ device_ids=\"[0, 2]\" \\ video_paths=\"[./sample/v_ZNVhz7ctTq0.mp4, ./sample/v_GGSY1Qvo990.mp4]\" See I3D Examples . Note, that this implementation of R(2+1)d only supports the RGB stream. Credits The TorchVision implementation . The R(2+1)D paper: A Closer Look at Spatiotemporal Convolutions for Action Recognition . Thanks to @ohjho we now also support the favors of the 34-layer model pre-trained on IG-65M and fine-tuned on Kinetics 400 . A shout-out to devs of moabitcoin/ig65m-pytorch who adapted weights of these favors from Caffe to PyTorch. The paper where these flavors were presented: Large-scale weakly-supervised pre-training for video action recognition License The wrapping code is under MIT, yet, it utilizes torchvision library which is under BSD 3-Clause \"New\" or \"Revised\" License .","title":"R(2+1)D (RGB-only)"},{"location":"models/r21d/#r21d-rgb-only","text":"We support 3 flavors of R(2+1)D: r2plus1d_18_16_kinetics 18-layer R(2+1)D pre-trained on Kinetics 400 (used by default) \u2013 it is identical to the torchvision implementation r2plus1d_34_32_ig65m_ft_kinetics 34-layer R(2+1)D pre-trained on IG-65M and fine-tuned on Kinetics 400 \u2013 the weights are provided by moabitcoin/ig65m-pytorch repo for stack/step size 32 . r2plus1d_34_8_ig65m_ft_kinetics the same as the one above but this one was pre-trained with stack/step size 8 models are pre-trained on RGB frames and follow the plain torchvision augmentation sequence . Info The flavors that were pre-trained on IG-65M and fine-tuned on Kinetics 400 yield significantly better performance than the default model (e.g. the 32 frame model reaches an accuracy of 79.10 vs 57.50 by default). By default ( --model_name=r2plus1d_18_16_kinetics ), the model expects to input a stack of 16 RGB frames ( 112x112 ), which spans 0.64 seconds of the video recorded at 25 fps. In the default case, the features will be of size Tv x 512 where Tv = duration / 0.64 . Specify, model_name , --step_size and --stack_size to change the default behavior.","title":"R(2+1)D (RGB-only)"},{"location":"models/r21d/#set-up-the-environment-for-r21d","text":"Setup conda environment. Requirements are in file conda_env_torch_zoo.yml # it will create a new conda environment called 'torch_zoo' on your machine conda env create -f conda_env_torch_zoo.yml","title":"Set up the Environment for R(2+1)D"},{"location":"models/r21d/#minimal-working-example","text":"Activate the environment conda activate torch_zoo and extract features from the ./sample/v_GGSY1Qvo990.mp4 video and show the predicted classes python main.py \\ feature_type=r21d \\ video_paths=\"[./sample/v_GGSY1Qvo990.mp4]\" \\ show_pred=true","title":"Minimal Working Example"},{"location":"models/r21d/#example","text":"Start by activating the environment conda activate torch_zoo It will extract R(2+1)d features for sample videos using 0th and 2nd devices in parallel. The features are going to be extracted with the default parameters. python main.py \\ feature_type=r21d \\ device_ids=\"[0, 2]\" \\ video_paths=\"[./sample/v_ZNVhz7ctTq0.mp4, ./sample/v_GGSY1Qvo990.mp4]\" See I3D Examples . Note, that this implementation of R(2+1)d only supports the RGB stream.","title":"Example"},{"location":"models/r21d/#credits","text":"The TorchVision implementation . The R(2+1)D paper: A Closer Look at Spatiotemporal Convolutions for Action Recognition . Thanks to @ohjho we now also support the favors of the 34-layer model pre-trained on IG-65M and fine-tuned on Kinetics 400 . A shout-out to devs of moabitcoin/ig65m-pytorch who adapted weights of these favors from Caffe to PyTorch. The paper where these flavors were presented: Large-scale weakly-supervised pre-training for video action recognition","title":"Credits"},{"location":"models/r21d/#license","text":"The wrapping code is under MIT, yet, it utilizes torchvision library which is under BSD 3-Clause \"New\" or \"Revised\" License .","title":"License"},{"location":"models/raft/","text":"RAFT Recurrent All-Pairs Field Transforms for Optical Flow (RAFT) frames are extracted for every consecutive pair of frames in a video. The implementation follows the official implementation . RAFT is pre-trained on FlyingChairs , fine-tuned on FlyingThings3D , then it is finetuned on Sintel or KITTI-2015 (see the Training Schedule in the Experiments section in the RAFT paper). Also, check out and this issue to learn more about the shared models. The optical flow frames have the same size as the video input or as specified by the resize arguments. We additionally output timesteps in ms for each feature and fps of the video. Set up the Environment for RAFT Setup conda environment. Requirements for RAFT are similar to the torchvision zoo, which uses conda_env_torch_zoo.yml # it will create a new conda environment called 'torch_zoo' on your machine conda env create -f conda_env_torch_zoo.yml Minimal Working Example Activate the environment conda activate torch_zoo and extract optical flow from ./sample/v_GGSY1Qvo990.mp4 using one GPU and show the flow for each frame python main.py \\ feature_type=raft \\ show_pred=true \\ video_paths=\"[./sample/v_GGSY1Qvo990.mp4]\" Note , if show_pred=true , the window with predictions will appear, use any key to show the next frame. To use show_pred=true , a screen must be attached to the machine or X11 forwarding is enabled. Examples Start by activating the environment conda activate torch_zoo A minimal working example: it will extract RAFT optical flow frames for sample videos using 0th and 2nd devices in parallel. python main.py \\ feature_type=raft \\ device_ids=\"[0, 2]\" \\ video_paths=\"[./sample/v_ZNVhz7ctTq0.mp4, ./sample/v_GGSY1Qvo990.mp4]\" Note, if your videos are quite long, have large dimensions and fps, watch your RAM as the frames are stored in the memory until they are saved. Please see other examples how can you overcome this problem. By default, the frames are extracted using the Sintel model. If you wish you can use KITTI-pretrained model by changing the finetuned_on argument: python main.py \\ feature_type=raft \\ device_ids=\"[0, 2]\" \\ finetuned_on=kitti \\ video_paths=\"[./sample/v_ZNVhz7ctTq0.mp4, ./sample/v_GGSY1Qvo990.mp4]\" If you would like to save the frames, use --on_extraction save_numpy (or save_pickle ) \u2013 by default, the frames are saved in ./output/ or where --output_path specifies. In the case of RAFT, besides frames, it also saves timestamps in ms and the original fps of the video into the same folder with features. python main.py \\ feature_type=raft \\ device_ids=\"[0, 2]\" \\ on_extraction=save_numpy \\ video_paths=\"[./sample/v_ZNVhz7ctTq0.mp4, ./sample/v_GGSY1Qvo990.mp4]\" Since extracting flow between two frames is cheap we may increase the extraction speed with batching. Therefore, you can use --batch_size argument (defaults to 1 ) to do so. A precaution: make sure to properly test the memory impact of using a specific batch size if you are not sure which kind of videos you have. For instance, you tested the extraction on 16:9 aspect ratio videos but some videos are 16:10 which might give you a mem error. Therefore, I would recommend to tune --batch_size on a square video and using the resize arguments (showed later) python main.py \\ feature_type=raft \\ device_ids=\"[0, 2]\" \\ batch_size=16 \\ video_paths=\"[./sample/v_ZNVhz7ctTq0.mp4, ./sample/v_GGSY1Qvo990.mp4]\" Another way of speeding up the extraction is to resize the input frames. Use resize_to_smaller_edge=true (default) if you would like --side_size to be min(W, H) if resize_to_smaller_edge=false the --side_size value will correspond to be max(W, H) . The latter might be useful when you are not sure which aspect ratio the videos have (the upper bound on size). python main.py \\ feature_type=raft \\ device_ids=\"[0, 2]\" \\ side_size=256 \\ resize_to_smaller_edge=false \\ video_paths=\"[./sample/v_ZNVhz7ctTq0.mp4, ./sample/v_GGSY1Qvo990.mp4]\" If the videos have different fps rate, --extraction_fps might be used to specify the target fps of all videos (a video is reencoded and saved to --tmp_path folder and deleted if --keep_tmp_files wasn't used). python main.py \\ feature_type=raft \\ device_ids=\"[0, 2]\" \\ extraction_fps=1 \\ video_paths=\"[./sample/v_ZNVhz7ctTq0.mp4, ./sample/v_GGSY1Qvo990.mp4]\" Credits The Official RAFT implementation (esp. ./demo.py ) . The RAFT paper: RAFT: Recurrent All Pairs Field Transforms for Optical Flow . License The wrapping code is under MIT, but the RAFT implementation complies with BSD 3-Clause .","title":"RAFT"},{"location":"models/raft/#raft","text":"Recurrent All-Pairs Field Transforms for Optical Flow (RAFT) frames are extracted for every consecutive pair of frames in a video. The implementation follows the official implementation . RAFT is pre-trained on FlyingChairs , fine-tuned on FlyingThings3D , then it is finetuned on Sintel or KITTI-2015 (see the Training Schedule in the Experiments section in the RAFT paper). Also, check out and this issue to learn more about the shared models. The optical flow frames have the same size as the video input or as specified by the resize arguments. We additionally output timesteps in ms for each feature and fps of the video.","title":"RAFT"},{"location":"models/raft/#set-up-the-environment-for-raft","text":"Setup conda environment. Requirements for RAFT are similar to the torchvision zoo, which uses conda_env_torch_zoo.yml # it will create a new conda environment called 'torch_zoo' on your machine conda env create -f conda_env_torch_zoo.yml","title":"Set up the Environment for RAFT"},{"location":"models/raft/#minimal-working-example","text":"Activate the environment conda activate torch_zoo and extract optical flow from ./sample/v_GGSY1Qvo990.mp4 using one GPU and show the flow for each frame python main.py \\ feature_type=raft \\ show_pred=true \\ video_paths=\"[./sample/v_GGSY1Qvo990.mp4]\" Note , if show_pred=true , the window with predictions will appear, use any key to show the next frame. To use show_pred=true , a screen must be attached to the machine or X11 forwarding is enabled.","title":"Minimal Working Example"},{"location":"models/raft/#examples","text":"Start by activating the environment conda activate torch_zoo A minimal working example: it will extract RAFT optical flow frames for sample videos using 0th and 2nd devices in parallel. python main.py \\ feature_type=raft \\ device_ids=\"[0, 2]\" \\ video_paths=\"[./sample/v_ZNVhz7ctTq0.mp4, ./sample/v_GGSY1Qvo990.mp4]\" Note, if your videos are quite long, have large dimensions and fps, watch your RAM as the frames are stored in the memory until they are saved. Please see other examples how can you overcome this problem. By default, the frames are extracted using the Sintel model. If you wish you can use KITTI-pretrained model by changing the finetuned_on argument: python main.py \\ feature_type=raft \\ device_ids=\"[0, 2]\" \\ finetuned_on=kitti \\ video_paths=\"[./sample/v_ZNVhz7ctTq0.mp4, ./sample/v_GGSY1Qvo990.mp4]\" If you would like to save the frames, use --on_extraction save_numpy (or save_pickle ) \u2013 by default, the frames are saved in ./output/ or where --output_path specifies. In the case of RAFT, besides frames, it also saves timestamps in ms and the original fps of the video into the same folder with features. python main.py \\ feature_type=raft \\ device_ids=\"[0, 2]\" \\ on_extraction=save_numpy \\ video_paths=\"[./sample/v_ZNVhz7ctTq0.mp4, ./sample/v_GGSY1Qvo990.mp4]\" Since extracting flow between two frames is cheap we may increase the extraction speed with batching. Therefore, you can use --batch_size argument (defaults to 1 ) to do so. A precaution: make sure to properly test the memory impact of using a specific batch size if you are not sure which kind of videos you have. For instance, you tested the extraction on 16:9 aspect ratio videos but some videos are 16:10 which might give you a mem error. Therefore, I would recommend to tune --batch_size on a square video and using the resize arguments (showed later) python main.py \\ feature_type=raft \\ device_ids=\"[0, 2]\" \\ batch_size=16 \\ video_paths=\"[./sample/v_ZNVhz7ctTq0.mp4, ./sample/v_GGSY1Qvo990.mp4]\" Another way of speeding up the extraction is to resize the input frames. Use resize_to_smaller_edge=true (default) if you would like --side_size to be min(W, H) if resize_to_smaller_edge=false the --side_size value will correspond to be max(W, H) . The latter might be useful when you are not sure which aspect ratio the videos have (the upper bound on size). python main.py \\ feature_type=raft \\ device_ids=\"[0, 2]\" \\ side_size=256 \\ resize_to_smaller_edge=false \\ video_paths=\"[./sample/v_ZNVhz7ctTq0.mp4, ./sample/v_GGSY1Qvo990.mp4]\" If the videos have different fps rate, --extraction_fps might be used to specify the target fps of all videos (a video is reencoded and saved to --tmp_path folder and deleted if --keep_tmp_files wasn't used). python main.py \\ feature_type=raft \\ device_ids=\"[0, 2]\" \\ extraction_fps=1 \\ video_paths=\"[./sample/v_ZNVhz7ctTq0.mp4, ./sample/v_GGSY1Qvo990.mp4]\"","title":"Examples"},{"location":"models/raft/#credits","text":"The Official RAFT implementation (esp. ./demo.py ) . The RAFT paper: RAFT: Recurrent All Pairs Field Transforms for Optical Flow .","title":"Credits"},{"location":"models/raft/#license","text":"The wrapping code is under MIT, but the RAFT implementation complies with BSD 3-Clause .","title":"License"},{"location":"models/resnet/","text":"ResNet The ResNet features are extracted at each frame of the provided video. The ResNet is pre-trained on the 1k ImageNet dataset. We extract features from the pre-classification layer. The implementation is based on the torchvision models . The extracted features are going to be of size num_frames x 2048 . We additionally output timesteps in ms for each feature and fps of the video. We use the standard set of augmentations. Set up the Environment for ResNet Setup conda environment. Requirements are in file conda_env_torch_zoo.yml # it will create a new conda environment called 'torch_zoo' on your machine conda env create -f conda_env_torch_zoo.yml Minimal Working Example Activate the environment conda activate torch_zoo and extract features at 1 fps from ./sample/v_GGSY1Qvo990.mp4 video and show the predicted classes python main.py \\ feature_type=resnet101 \\ extraction_fps=1 \\ video_paths=\"[./sample/v_GGSY1Qvo990.mp4]\" \\ show_pred=true Examples Start by activating the environment conda activate torch_zoo It is pretty much the same procedure as with other features. The example is provided for the ResNet-50 flavour, but we also support ResNet-18,34,101,152. python main.py \\ feature_type=resnet50 \\ device_ids=\"[0, 2]\" \\ video_paths=\"[./sample/v_ZNVhz7ctTq0.mp4, ./sample/v_GGSY1Qvo990.mp4]\" If you would like to save the features, use --on_extraction save_numpy (or save_pickle ) \u2013 by default, the features are saved in ./output/ or where --output_path specifies. In the case of frame-wise features, besides features, it also saves timestamps in ms and the original fps of the video into the same folder with features. python main.py \\ feature_type=resnet50 \\ device_ids=\"[0, 2]\" \\ on_extraction=save_numpy \\ file_with_video_paths=./sample/sample_video_paths.txt Since these features are so fine-grained and light-weight we may increase the extraction speed with batching. Therefore, frame-wise features have --batch_size argument, which defaults to 1 . python main.py \\ feature_type=resnet50 \\ device_ids=\"[0, 2]\" \\ batch_size=128 \\ video_paths=\"[./sample/v_ZNVhz7ctTq0.mp4, ./sample/v_GGSY1Qvo990.mp4]\" If you would like to extract features at a certain fps, add --extraction_fps argument python main.py \\ feature_type=resnet50 \\ device_ids=\"[0, 2]\" \\ extraction_fps=5 \\ video_paths=\"[./sample/v_ZNVhz7ctTq0.mp4, ./sample/v_GGSY1Qvo990.mp4]\" Credits The TorchVision implementation . The ResNet paper License The wrapping code is under MIT, yet, it utilizes torchvision library which is under BSD 3-Clause \"New\" or \"Revised\" License .","title":"ResNet"},{"location":"models/resnet/#resnet","text":"The ResNet features are extracted at each frame of the provided video. The ResNet is pre-trained on the 1k ImageNet dataset. We extract features from the pre-classification layer. The implementation is based on the torchvision models . The extracted features are going to be of size num_frames x 2048 . We additionally output timesteps in ms for each feature and fps of the video. We use the standard set of augmentations.","title":"ResNet"},{"location":"models/resnet/#set-up-the-environment-for-resnet","text":"Setup conda environment. Requirements are in file conda_env_torch_zoo.yml # it will create a new conda environment called 'torch_zoo' on your machine conda env create -f conda_env_torch_zoo.yml","title":"Set up the Environment for ResNet"},{"location":"models/resnet/#minimal-working-example","text":"Activate the environment conda activate torch_zoo and extract features at 1 fps from ./sample/v_GGSY1Qvo990.mp4 video and show the predicted classes python main.py \\ feature_type=resnet101 \\ extraction_fps=1 \\ video_paths=\"[./sample/v_GGSY1Qvo990.mp4]\" \\ show_pred=true","title":"Minimal Working Example"},{"location":"models/resnet/#examples","text":"Start by activating the environment conda activate torch_zoo It is pretty much the same procedure as with other features. The example is provided for the ResNet-50 flavour, but we also support ResNet-18,34,101,152. python main.py \\ feature_type=resnet50 \\ device_ids=\"[0, 2]\" \\ video_paths=\"[./sample/v_ZNVhz7ctTq0.mp4, ./sample/v_GGSY1Qvo990.mp4]\" If you would like to save the features, use --on_extraction save_numpy (or save_pickle ) \u2013 by default, the features are saved in ./output/ or where --output_path specifies. In the case of frame-wise features, besides features, it also saves timestamps in ms and the original fps of the video into the same folder with features. python main.py \\ feature_type=resnet50 \\ device_ids=\"[0, 2]\" \\ on_extraction=save_numpy \\ file_with_video_paths=./sample/sample_video_paths.txt Since these features are so fine-grained and light-weight we may increase the extraction speed with batching. Therefore, frame-wise features have --batch_size argument, which defaults to 1 . python main.py \\ feature_type=resnet50 \\ device_ids=\"[0, 2]\" \\ batch_size=128 \\ video_paths=\"[./sample/v_ZNVhz7ctTq0.mp4, ./sample/v_GGSY1Qvo990.mp4]\" If you would like to extract features at a certain fps, add --extraction_fps argument python main.py \\ feature_type=resnet50 \\ device_ids=\"[0, 2]\" \\ extraction_fps=5 \\ video_paths=\"[./sample/v_ZNVhz7ctTq0.mp4, ./sample/v_GGSY1Qvo990.mp4]\"","title":"Examples"},{"location":"models/resnet/#credits","text":"The TorchVision implementation . The ResNet paper","title":"Credits"},{"location":"models/resnet/#license","text":"The wrapping code is under MIT, yet, it utilizes torchvision library which is under BSD 3-Clause \"New\" or \"Revised\" License .","title":"License"},{"location":"models/vggish/","text":"VGGish The VGGish feature extraction relies on the PyTorch implementation by harritaylor built to replicate the procedure provided in the TensorFlow repository . The difference in values between the PyTorch and Tensorflow implementation is negligible (see also # difference in values ). The VGGish model was pre-trained on AudioSet . The extracted features are from pre-classification layer after activation. The feature tensor will be 128-d and correspond to 0.96 sec of the original video. Interestingly, this might be represented as 24 frames of a 25 fps video. Therefore, you should expect Ta x 128 features, where Ta = duration / 0.96 . The extraction of VGGish features is implemeted as a wrapper of the TensorFlow implementation. See Credits . Set up the Environment for VGGish Setup conda environment. Requirements are in file conda_env_torch_zoo.yml # it will create a new conda environment called 'torch_zoo' on your machine conda env create -f conda_env_torch_zoo.yml Minimal Working Example Activate the environment conda activate torch_zoo and extract features from the ./sample/v_GGSY1Qvo990.mp4 video python main.py \\ feature_type=vggish \\ video_paths=\"[./sample/v_GGSY1Qvo990.mp4]\" Example The video paths can be specified as a .txt file with paths. python main.py \\ feature_type=vggish \\ device_ids=\"[0, 2]\" \\ file_with_video_paths=./sample/sample_video_paths.txt The features can be saved as numpy arrays by specifying --on_extraction save_numpy or save_pickle . By default, it will create a folder ./output and will store features there (you can change the output folder using --output_path ) python main.py \\ feature_type=vggish \\ device_ids=\"[0, 2]\" \\ on_extraction=save_numpy \\ video_paths=\"[./sample/v_ZNVhz7ctTq0.mp4, ./sample/v_GGSY1Qvo990.mp4]\" Difference between Tensorflow and PyTorch implementations python main.py \\ feature_type=vggish \\ on_extraction=save_numpy \\ file_with_video_paths=./sample/sample_video_paths.txt TF (./sample/v_GGSY1Qvo990.mp4): [[0. 0.04247099 0.09079538 ... 0. 0.18485409 0. ] [0. 0. 0. ... 0. 0.5720243 0.5475726 ] [0. 0.00705254 0.15173683 ... 0. 0.33540994 0.10572422] ... [0. 0. 0.36020872 ... 0. 0.08559107 0.00870359] [0. 0.21485361 0.16507196 ... 0. 0. 0. ] [0. 0.31638345 0. ... 0. 0. 0. ]] max: 2.31246495; mean: 0.13741589; min: 0.00000000 PyTorch (./sample/v_GGSY1Qvo990.mp4): [[0. 0.04247095 0.09079528 ... 0. 0.18485469 0. ] [0. 0. 0. ... 0. 0.5720252 0.5475726 ] [0. 0.0070536 0.1517372 ... 0. 0.33541012 0.10572463] ... [0. 0. 0.36020786 ... 0. 0.08559084 0.00870359] [0. 0.21485506 0.16507116 ... 0. 0. 0. ] [0. 0.31638315 0. ... 0. 0. 0. ]] max: 2.31246495; mean: 0.13741589; min: 0.00000000 (PyTorch - TensorFlow).abs() tensor([[0.0000e+00, 4.4703e-08, 1.0431e-07, ..., 0.0000e+00, 5.9605e-07, 0.0000e+00], [0.0000e+00, 0.0000e+00, 0.0000e+00, ..., 0.0000e+00, 8.9407e-07, 0.0000e+00], [0.0000e+00, 1.0580e-06, 3.7253e-07, ..., 0.0000e+00, 1.7881e-07, 4.1723e-07], ..., [0.0000e+00, 0.0000e+00, 8.6427e-07, ..., 0.0000e+00, 2.3097e-07, 0.0000e+00], [0.0000e+00, 1.4454e-06, 8.0466e-07, ..., 0.0000e+00, 0.0000e+00, 0.0000e+00], [0.0000e+00, 2.9802e-07, 0.0000e+00, ..., 0.0000e+00, 0.0000e+00, 0.0000e+00]]) max: 4.0531e-06; mean: 2.2185e-07; min: 0.00000000 Credits The PyTorch implementation of vggish . The VGGish paper: CNN Architectures for Large-Scale Audio Classification . License The wrapping code is under MIT but the vggish implementation complies with the harritaylor/torchvggish (same as tensorflow) license which is Apache-2.0 .","title":"VGGish"},{"location":"models/vggish/#vggish","text":"The VGGish feature extraction relies on the PyTorch implementation by harritaylor built to replicate the procedure provided in the TensorFlow repository . The difference in values between the PyTorch and Tensorflow implementation is negligible (see also # difference in values ). The VGGish model was pre-trained on AudioSet . The extracted features are from pre-classification layer after activation. The feature tensor will be 128-d and correspond to 0.96 sec of the original video. Interestingly, this might be represented as 24 frames of a 25 fps video. Therefore, you should expect Ta x 128 features, where Ta = duration / 0.96 . The extraction of VGGish features is implemeted as a wrapper of the TensorFlow implementation. See Credits .","title":"VGGish"},{"location":"models/vggish/#set-up-the-environment-for-vggish","text":"Setup conda environment. Requirements are in file conda_env_torch_zoo.yml # it will create a new conda environment called 'torch_zoo' on your machine conda env create -f conda_env_torch_zoo.yml","title":"Set up the Environment for VGGish"},{"location":"models/vggish/#minimal-working-example","text":"Activate the environment conda activate torch_zoo and extract features from the ./sample/v_GGSY1Qvo990.mp4 video python main.py \\ feature_type=vggish \\ video_paths=\"[./sample/v_GGSY1Qvo990.mp4]\"","title":"Minimal Working Example"},{"location":"models/vggish/#example","text":"The video paths can be specified as a .txt file with paths. python main.py \\ feature_type=vggish \\ device_ids=\"[0, 2]\" \\ file_with_video_paths=./sample/sample_video_paths.txt The features can be saved as numpy arrays by specifying --on_extraction save_numpy or save_pickle . By default, it will create a folder ./output and will store features there (you can change the output folder using --output_path ) python main.py \\ feature_type=vggish \\ device_ids=\"[0, 2]\" \\ on_extraction=save_numpy \\ video_paths=\"[./sample/v_ZNVhz7ctTq0.mp4, ./sample/v_GGSY1Qvo990.mp4]\"","title":"Example"},{"location":"models/vggish/#difference-between-tensorflow-and-pytorch-implementations","text":"python main.py \\ feature_type=vggish \\ on_extraction=save_numpy \\ file_with_video_paths=./sample/sample_video_paths.txt TF (./sample/v_GGSY1Qvo990.mp4): [[0. 0.04247099 0.09079538 ... 0. 0.18485409 0. ] [0. 0. 0. ... 0. 0.5720243 0.5475726 ] [0. 0.00705254 0.15173683 ... 0. 0.33540994 0.10572422] ... [0. 0. 0.36020872 ... 0. 0.08559107 0.00870359] [0. 0.21485361 0.16507196 ... 0. 0. 0. ] [0. 0.31638345 0. ... 0. 0. 0. ]] max: 2.31246495; mean: 0.13741589; min: 0.00000000 PyTorch (./sample/v_GGSY1Qvo990.mp4): [[0. 0.04247095 0.09079528 ... 0. 0.18485469 0. ] [0. 0. 0. ... 0. 0.5720252 0.5475726 ] [0. 0.0070536 0.1517372 ... 0. 0.33541012 0.10572463] ... [0. 0. 0.36020786 ... 0. 0.08559084 0.00870359] [0. 0.21485506 0.16507116 ... 0. 0. 0. ] [0. 0.31638315 0. ... 0. 0. 0. ]] max: 2.31246495; mean: 0.13741589; min: 0.00000000 (PyTorch - TensorFlow).abs() tensor([[0.0000e+00, 4.4703e-08, 1.0431e-07, ..., 0.0000e+00, 5.9605e-07, 0.0000e+00], [0.0000e+00, 0.0000e+00, 0.0000e+00, ..., 0.0000e+00, 8.9407e-07, 0.0000e+00], [0.0000e+00, 1.0580e-06, 3.7253e-07, ..., 0.0000e+00, 1.7881e-07, 4.1723e-07], ..., [0.0000e+00, 0.0000e+00, 8.6427e-07, ..., 0.0000e+00, 2.3097e-07, 0.0000e+00], [0.0000e+00, 1.4454e-06, 8.0466e-07, ..., 0.0000e+00, 0.0000e+00, 0.0000e+00], [0.0000e+00, 2.9802e-07, 0.0000e+00, ..., 0.0000e+00, 0.0000e+00, 0.0000e+00]]) max: 4.0531e-06; mean: 2.2185e-07; min: 0.00000000","title":"Difference between Tensorflow and PyTorch implementations"},{"location":"models/vggish/#credits","text":"The PyTorch implementation of vggish . The VGGish paper: CNN Architectures for Large-Scale Audio Classification .","title":"Credits"},{"location":"models/vggish/#license","text":"The wrapping code is under MIT but the vggish implementation complies with the harritaylor/torchvggish (same as tensorflow) license which is Apache-2.0 .","title":"License"}]}